{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b4717a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdf882",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e5704",
   "metadata": {},
   "source": [
    "- 문장토큰화</br>\n",
    "?나 !는 꽤 명확한 구분자지만 .의 경우 절대적이지 않음</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cc50ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-05T23:57:43.246288Z",
     "start_time": "2021-08-05T23:57:41.013219Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aba9786b35ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize  # 영어\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb95fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-05T23:57:43.249290Z",
     "start_time": "2021-08-05T23:57:41.013Z"
    }
   },
   "outputs": [],
   "source": [
    "import kss  # 한글\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617272b8",
   "metadata": {},
   "source": [
    "구두점과 특수문자를 단순제외하면 안됨(문장경계표시, 단어자체에서 ph.D나 02-23처럼)</br>\n",
    "줄임말이나 단어 내 띄어쓰기(you're이나 new york처럼)</br>\n",
    "게다가 영어는 위만 잘해낸다면 whitespace기준 띄어쓰기 토큰화도 잘 되지만</br>\n",
    "1.한글은 조사가 있음/2.띄어쓰기가 필수아님</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(text)   # 단어토큰화\n",
    "print(pos_tag(words))  # 품사태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a923d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-05T23:57:43.256296Z",
     "start_time": "2021-08-05T23:57:41.027Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "words = okt.morphs(text)  # 단어토큰화\n",
    "print(okt.pos(text))  # 품사태김(품사따라서 의미가 달라짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f9f9a",
   "metadata": {},
   "source": [
    "keras 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578e79a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-05T23:57:43.252292Z",
     "start_time": "2021-08-05T23:57:41.025Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "Tokenizer()\n",
    "t.fit_on_texts([\"the earth is an awesome place to live\"])  # text들로 만드는 객체\n",
    "sequences = t.texts_to_sequences([\"the earth is great place to live\"])  # sequence란 수열\n",
    "print(t.word_index)   # 단어집합 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6a77e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-05T23:57:43.254292Z",
     "start_time": "2021-08-05T23:57:41.026Z"
    }
   },
   "outputs": [],
   "source": [
    "print(t.texts_to_matrix(texts, mode = 'count'))  # 문서단어행렬(word_index가 1부터 부여되는 바람에 첫열은 상관없음)\n",
    "print(t.texts_to_matrix(texts, mode = 'tfidf').round(2)) # 빈도에 자연로그씌우고 1더함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242c3f2",
   "metadata": {},
   "source": [
    "### Cleansing and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a27bfc",
   "metadata": {},
   "source": [
    "cleansing은 노이즈 제거, normalization은 표현방법 다른 단어를 같은 단어로 만들기</br>\n",
    "1. 규칙\n",
    "2. 대소문자통합\n",
    "3. 불필요한 단어의 제거\n",
    "4. 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ee0bf",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da422df0",
   "metadata": {},
   "source": [
    "코퍼스속 단어의 개수를 줄인다(빈도수중요한 BoW기법에서 사용)</br>\n",
    "lemma는 원형 복원(품사반영)</br>\n",
    "stem(어간)과 affix(접사)중 어간을 추출(차원축소)</br>\n",
    "영어는 둘이 명확하게 달라서 선택해야하고 한글은 형태소분석과정에서 같이하는 셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  # 대표격\n",
    "s = PorterStemmer()\n",
    "result = [s.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "result = [n.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b905dc",
   "metadata": {},
   "source": [
    "## Stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c23f4",
   "metadata": {},
   "source": [
    "코퍼스속 큰 의미 없는 단어 제거</br>\n",
    "자주 등장하지만 큰 의미없는 단어들(패키지에서 정의하기도 하고 설정해줄 수 있음)</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7bf03",
   "metadata": {},
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5d9f8",
   "metadata": {},
   "source": [
    "# Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23e2a6",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "vocab = FreqDist(np.hstack(sentences))  # 이중 리스트를 리스트로 변환하여 (문장 구분을 제거하여) 입력\n",
    "vocab = vocab.most_common(5) # 빈도 수가 높은 상위 5개 단어만을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23b601",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a111a0",
   "metadata": {},
   "source": [
    "긴 건 자르고 짧은 건 0으로 채움</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281b2829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-05T23:59:57.526668Z",
     "start_time": "2021-08-05T23:59:54.328394Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [0, 7, 8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre')  # 앞에 0채움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654832c2",
   "metadata": {},
   "source": [
    "## One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "저장 공간 측면에서는 매우 비효율적</br>\n",
    "단어의 유사도를 표현하지 못한다는 단점</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa63d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        \n",
    "        \n",
    "# 토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 반환하는 함수\n",
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index)) # 단어 집합의 크기가 벡터의 차원, 모든 값을 0으로 초기화\n",
    "    index = word2index[word] # 표현하고 싶은 단어의 인덱스\n",
    "    one_hot_vector[index] = 1 # 표현하고 싶은 단어의 인덱스 위치에 1을 값으로 부여\n",
    "    return one_hot_vector # 원-핫 벡터를 반환\n",
    "\n",
    "print(one_hot_encoding('자연어', word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4accd",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c2311",
   "metadata": {},
   "source": [
    "중심단어와 윈도우 거리의 주변단어들의 의미를 참고해 단어를 인코딩\n",
    "1. CBOW방식 주변단어로 중심단어 예측 \n",
    "2. skip-gram 중심단어로 주변단어 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d0109",
   "metadata": {},
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(stopwords = stopwords,collocations = False) # collocations=True 이면 said King같이 자주 나타나는 단어는 하나의 어구로 분류\n",
    "wc.generate(alice_text)  # alice_novel이라는 텍스트 데이터를 사용하여 wordcloud를 generate하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "alicewc.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud를 이미지로 저장하기\n",
    "alicewc.to_file(\"my_alice.png\") # 다른 확장자로도 저장 가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
